"""
ç¤ºä¾‹è„šæœ¬ï¼šå±•ç¤ºå¦‚ä½•ä½¿ç”¨analysisåŒ…è¿›è¡Œæ•°æ®åˆ†æ
"""

import daft
import pandas as pd
import numpy as np
import os
from mdgp_processors import (
    DataAnalyzer,
    DataVisualizer,
    EvaluationAnalyzer,
    TextQualityEvaluator,
    DataPipeline
)

# åˆ›å»ºç¤ºä¾‹æ•°æ®
def create_sample_data():
    """åˆ›å»ºåŒ…å«è¯„ä¼°ç»“æœçš„ç¤ºä¾‹æ•°æ®"""
    np.random.seed(42)
    
    # åˆ›å»ºæ–‡æœ¬æ•°æ®
    texts = [
        "è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬æ ·æœ¬ï¼Œå†…å®¹ä¸°å¯Œä¸”ç»“æ„æ¸…æ™°ã€‚",
        "ç®€çŸ­æ–‡æœ¬ã€‚",
        "è´¨é‡ä¸€èˆ¬çš„æ–‡æœ¬ï¼Œæ²¡æœ‰ç‰¹åˆ«çš„äº®ç‚¹ã€‚",
        "éå¸¸å¥½çš„æ–‡æœ¬ï¼è¯¦ç»†ä»‹ç»äº†ä¸»é¢˜å†…å®¹ã€‚",
        "è¾ƒå·®çš„æ–‡æœ¬ï¼Œå†…å®¹ä¸å®Œæ•´ã€‚",
        "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰è´¨é‡çš„æ–‡æœ¬ï¼Œæœ‰ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ã€‚",
        "ä¼˜ç§€çš„æ–‡æœ¬ï¼Œé€»è¾‘ä¸¥è°¨ï¼Œè¡¨è¾¾æµç•…ã€‚",
        "ç³Ÿç³•çš„æ–‡æœ¬ï¼Œå‡ ä¹æ²¡æœ‰å®è´¨å†…å®¹ã€‚",
        "æ™®é€šæ–‡æœ¬ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ã€‚",
        "å®Œç¾çš„æ–‡æœ¬ï¼Œå„æ–¹é¢éƒ½å¾ˆå‡ºè‰²ã€‚"
    ] * 20  # 200ä¸ªæ ·æœ¬
    
    # åˆ›å»ºæ•°æ®æ¡†
    data = {
        "text": texts,
        "length": [len(text) for text in texts],
        "category": np.random.choice(["ç§‘æŠ€", "å¨±ä¹", "æ•™è‚²", "æ–°é—»"], size=200),
        "eval_text_quality": np.clip(np.random.normal(0.7, 0.15, 200), 0, 1),
        "eval_readability": np.clip(np.random.normal(0.65, 0.12, 200), 0, 1),
        "eval_coherence": np.clip(np.random.normal(0.72, 0.10, 200), 0, 1),
        "eval_relevance": np.clip(np.random.normal(0.68, 0.13, 200), 0, 1)
    }
    
    df = pd.DataFrame(data)
    return daft.from_pandas(df)

# ä¸»å‡½æ•°
def main():
    print("ğŸš€ å¼€å§‹æ•°æ®åˆ†æç¤ºä¾‹")
    print("=" * 50)
    
    # 1. åˆ›å»ºç¤ºä¾‹æ•°æ®
    print("\nğŸ“‹ 1. åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    daft_df = create_sample_data()
    pandas_df = daft_df.to_pandas()
    print(f"âœ… æ•°æ®åˆ›å»ºæˆåŠŸï¼ŒåŒ…å« {len(pandas_df)} è¡Œæ ·æœ¬")
    print(f"   åˆ—å: {list(pandas_df.columns)}")
    
    # 2. ä½¿ç”¨DataAnalyzeråˆ†ææ•°æ®åˆ†å¸ƒ
    print("\nğŸ“Š 2. ä½¿ç”¨DataAnalyzeråˆ†ææ•°æ®åˆ†å¸ƒ...")
    data_analyzer = DataAnalyzer(daft_df)
    
    # åˆ†æå•ä¸ªè¯„ä¼°åˆ—
    quality_stats = data_analyzer.analyze_column_distribution("eval_text_quality")
    print(f"\nğŸ“ˆ æ–‡æœ¬è´¨é‡è¯„ä¼°åˆ—åˆ†æ:")
    print(f"   å¹³å‡å€¼: {quality_stats['mean']:.2f}")
    print(f"   ä¸­ä½æ•°: {quality_stats['median']:.2f}")
    print(f"   æœ€å°å€¼: {quality_stats['min']:.2f}")
    print(f"   æœ€å¤§å€¼: {quality_stats['max']:.2f}")
    print(f"   æ ‡å‡†å·®: {quality_stats['std']:.2f}")
    
    # åˆ†ææ‰€æœ‰è¯„ä¼°åˆ—
    all_eval_stats = data_analyzer.analyze_evaluation_columns()
    print(f"\nğŸ“Š æ‰€æœ‰è¯„ä¼°åˆ—åˆ†æå®Œæˆï¼Œå…± {len(all_eval_stats)} ä¸ªè¯„ä¼°åˆ—")
    
    # 3. ä½¿ç”¨DataVisualizerç”Ÿæˆå›¾è¡¨
    print("\nğŸ¨ 3. ä½¿ç”¨DataVisualizerç”Ÿæˆå›¾è¡¨...")
    visualizer = DataVisualizer(pandas_df)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "analysis_results"
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆç›´æ–¹å›¾
    visualizer.plot_histogram("eval_text_quality",
                            title="æ–‡æœ¬è´¨é‡è¯„ä¼°åˆ†å¸ƒç›´æ–¹å›¾",
                            save_path=os.path.join(output_dir, "text_quality_histogram.png"))
    print(f"âœ… ç›´æ–¹å›¾å·²ä¿å­˜åˆ°: {os.path.join(output_dir, 'text_quality_histogram.png')}")
    
    # ç”Ÿæˆç®±çº¿å›¾
    visualizer.plot_boxplot("eval_readability",
                          title="å¯è¯»æ€§è¯„ä¼°ç®±çº¿å›¾",
                          save_path=os.path.join(output_dir, "readability_boxplot.png"))
    print(f"âœ… ç®±çº¿å›¾å·²ä¿å­˜åˆ°: {os.path.join(output_dir, 'readability_boxplot.png')}")
    
    # ç”Ÿæˆç›¸å…³ç³»æ•°çƒ­åŠ›å›¾
    visualizer.plot_correlation_heatmap(
        title="è¯„ä¼°åˆ—ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾",
        save_path=os.path.join(output_dir, "correlation_heatmap.png")
    )
    print(f"âœ… ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾å·²ä¿å­˜åˆ°: {os.path.join(output_dir, 'correlation_heatmap.png')}")
    
    # 4. ä½¿ç”¨EvaluationAnalyzeråˆ†æè¯„ä¼°ç»“æœ
    print("\nğŸ“ˆ 4. ä½¿ç”¨EvaluationAnalyzeråˆ†æè¯„ä¼°ç»“æœ...")
    eval_analyzer = EvaluationAnalyzer(daft_df)
    
    # è®¡ç®—é€šè¿‡ç‡
    pass_rates = eval_analyzer.calculate_all_pass_rates(threshold=0.7)
    print(f"\nğŸ“Š å„è¯„ä¼°åˆ—é€šè¿‡ç‡ (é˜ˆå€¼: 0.7):")
    for col, stats in pass_rates.items():
        print(f"   {col}: {stats['pass_rate']:.1f}%")
    
    # 5. ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š
    print("\nğŸ“‹ 5. ç”Ÿæˆå®Œæ•´è¯„ä¼°æŠ¥å‘Š...")
    report_dir = os.path.join(output_dir, "evaluation_report")
    eval_analyzer.generate_evaluation_report(report_dir)
    
    # 6. æ•°æ®åˆ†æå®Œæˆ
    print("\nğŸ‰ æ•°æ®åˆ†æç¤ºä¾‹å®Œæˆï¼")
    print("=" * 50)
    print(f"ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir} ç›®å½•")

if __name__ == "__main__":
    main()