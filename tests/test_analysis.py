"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯analysisåŒ…çš„åŠŸèƒ½
"""

import daft
import pandas as pd
import numpy as np
import os
import tempfile
from mdgp_processors import (
    DataAnalyzer,
    DataVisualizer,
    EvaluationAnalyzer,
    TextQualityEvaluator,
    DataPipeline
)

# åˆ›å»ºæµ‹è¯•æ•°æ®
def create_test_data():
    # åˆ›å»ºåŒ…å«æ–‡æœ¬è´¨é‡è¯„ä¼°ç»“æœçš„æµ‹è¯•æ•°æ®
    np.random.seed(42)
    
    data = {
        "text": [
            "è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬æ ·æœ¬ï¼Œå†…å®¹ä¸°å¯Œä¸”ç»“æ„æ¸…æ™°ã€‚",
            "ç®€çŸ­æ–‡æœ¬ã€‚",
            "è´¨é‡ä¸€èˆ¬çš„æ–‡æœ¬ï¼Œæ²¡æœ‰ç‰¹åˆ«çš„äº®ç‚¹ã€‚",
            "éå¸¸å¥½çš„æ–‡æœ¬ï¼è¯¦ç»†ä»‹ç»äº†ä¸»é¢˜å†…å®¹ã€‚",
            "è¾ƒå·®çš„æ–‡æœ¬ï¼Œå†…å®¹ä¸å®Œæ•´ã€‚",
            "è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰è´¨é‡çš„æ–‡æœ¬ï¼Œæœ‰ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ã€‚",
            "ä¼˜ç§€çš„æ–‡æœ¬ï¼Œé€»è¾‘ä¸¥è°¨ï¼Œè¡¨è¾¾æµç•…ã€‚",
            "ç³Ÿç³•çš„æ–‡æœ¬ï¼Œå‡ ä¹æ²¡æœ‰å®è´¨å†…å®¹ã€‚",
            "æ™®é€šæ–‡æœ¬ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ã€‚",
            "å®Œç¾çš„æ–‡æœ¬ï¼Œå„æ–¹é¢éƒ½å¾ˆå‡ºè‰²ã€‚"
        ] * 10,  # å¤åˆ¶10æ¬¡ä»¥å¢åŠ æ ·æœ¬é‡
        "original_quality": np.random.normal(0.7, 0.1, 100),
        "length": np.random.randint(10, 200, 100)
    }
    
    # åˆ›å»ºPandas DataFrame
    df = pd.DataFrame(data)
    
    # æ·»åŠ ä¸€äº›è¯„ä¼°åˆ—
    df["eval_text_quality"] = np.clip(df["original_quality"] + np.random.normal(0, 0.05, 100), 0, 1)
    df["eval_readability"] = np.clip(0.6 + np.random.normal(0, 0.1, 100), 0, 1)
    df["eval_coherence"] = np.clip(0.7 + np.random.normal(0, 0.08, 100), 0, 1)
    
    # è½¬æ¢ä¸ºDaft DataFrame
    return daft.from_pandas(df)

# æµ‹è¯•DataAnalyzer
def test_data_analyzer():
    print("ğŸ“Š æµ‹è¯•DataAnalyzer...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    daft_df = create_test_data()
    
    # åˆ›å»ºDataAnalyzerå®ä¾‹
    analyzer = DataAnalyzer(daft_df)
    
    # åˆ†æå•ä¸ªåˆ—
    text_stats = analyzer.analyze_column_distribution("text")
    print(f"âœ… æ–‡æœ¬åˆ—åˆ†ææˆåŠŸï¼ŒåŒ…å« {len(text_stats)} ä¸ªç»Ÿè®¡æŒ‡æ ‡")
    
    # åˆ†ææ•°å€¼åˆ—
    quality_stats = analyzer.analyze_column_distribution("eval_text_quality")
    print(f"âœ… è´¨é‡è¯„ä¼°åˆ—åˆ†ææˆåŠŸï¼Œå¹³å‡å€¼: {quality_stats['mean']:.2f}")
    
    # åˆ†ææ‰€æœ‰åˆ—
    all_stats = analyzer.analyze_all_columns()
    print(f"âœ… æ‰€æœ‰åˆ—åˆ†ææˆåŠŸï¼Œå…±åˆ†æ {len(all_stats)} ä¸ªåˆ—")
    
    # æ£€æµ‹å¼‚å¸¸å€¼
    outliers = analyzer.detect_outliers("length", method="iqr")
    print(f"âœ… å¼‚å¸¸å€¼æ£€æµ‹æˆåŠŸï¼Œæ‰¾åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
    
    # è·å–ç›¸å…³ç³»æ•°çŸ©é˜µ
    corr_matrix = analyzer.get_correlation_matrix()
    print(f"âœ… ç›¸å…³ç³»æ•°çŸ©é˜µè®¡ç®—æˆåŠŸï¼Œå½¢çŠ¶: {corr_matrix.shape}")
    
    print("ğŸ‰ DataAnalyzeræµ‹è¯•é€šè¿‡ï¼")

# æµ‹è¯•DataVisualizer
def test_data_visualizer():
    print("\nğŸ¨ æµ‹è¯•DataVisualizer...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    daft_df = create_test_data()
    pandas_df = daft_df.to_pandas()
    
    # åˆ›å»ºDataVisualizerå®ä¾‹
    visualizer = DataVisualizer(pandas_df)
    
    # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜å›¾è¡¨
    with tempfile.TemporaryDirectory() as tmpdir:
        # æµ‹è¯•ç›´æ–¹å›¾
        fig_hist = visualizer.plot_histogram("eval_text_quality", 
                                           save_path=os.path.join(tmpdir, "histogram.png"))
        print(f"âœ… ç›´æ–¹å›¾ç»˜åˆ¶æˆåŠŸ")
        
        # æµ‹è¯•ç®±çº¿å›¾
        fig_box = visualizer.plot_boxplot("length", 
                                         save_path=os.path.join(tmpdir, "boxplot.png"))
        print(f"âœ… ç®±çº¿å›¾ç»˜åˆ¶æˆåŠŸ")
        
        # æµ‹è¯•æ•£ç‚¹å›¾
        fig_scatter = visualizer.plot_scatter("eval_text_quality", "eval_readability", 
                                             save_path=os.path.join(tmpdir, "scatter.png"))
        print(f"âœ… æ•£ç‚¹å›¾ç»˜åˆ¶æˆåŠŸ")
        
        # æµ‹è¯•ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾
        fig_heatmap = visualizer.plot_correlation_heatmap(
            save_path=os.path.join(tmpdir, "heatmap.png")
        )
        print(f"âœ… ç›¸å…³ç³»æ•°çƒ­åŠ›å›¾ç»˜åˆ¶æˆåŠŸ")
        
        # æµ‹è¯•æ¡å½¢å›¾
        # åˆ›å»ºä¸€ä¸ªåˆ†ç±»åˆ—ç”¨äºæµ‹è¯•æ¡å½¢å›¾
        pandas_df["category"] = np.random.choice(["A", "B", "C", "D"], size=len(pandas_df))
        visualizer = DataVisualizer(pandas_df)  # æ›´æ–°visualizer
        fig_bar = visualizer.plot_bar_chart("category", 
                                           save_path=os.path.join(tmpdir, "bar_chart.png"))
        print(f"âœ… æ¡å½¢å›¾ç»˜åˆ¶æˆåŠŸ")
        
        # æµ‹è¯•åˆ†å¸ƒæ¯”è¾ƒå›¾
        fig_compare = visualizer.plot_distribution_comparison(
            ["eval_text_quality", "eval_readability", "eval_coherence"],
            save_path=os.path.join(tmpdir, "distribution_comparison.png")
        )
        print(f"âœ… åˆ†å¸ƒæ¯”è¾ƒå›¾ç»˜åˆ¶æˆåŠŸ")
    
    print("ğŸ‰ DataVisualizeræµ‹è¯•é€šè¿‡ï¼")

# æµ‹è¯•EvaluationAnalyzer
def test_evaluation_analyzer():
    print("\nğŸ“ˆ æµ‹è¯•EvaluationAnalyzer...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    daft_df = create_test_data()
    
    # åˆ›å»ºEvaluationAnalyzerå®ä¾‹
    eval_analyzer = EvaluationAnalyzer(daft_df)
    
    # åˆ†æè¯„ä¼°åˆ—
    eval_stats = eval_analyzer.analyze_evaluation_columns()
    print(f"âœ… è¯„ä¼°åˆ—åˆ†ææˆåŠŸï¼Œå…±åˆ†æ {len(eval_stats)} ä¸ªè¯„ä¼°åˆ—")
    
    # è®¡ç®—é€šè¿‡ç‡
    pass_rate = eval_analyzer.calculate_pass_rate("eval_text_quality", threshold=0.7)
    print(f"âœ… é€šè¿‡ç‡è®¡ç®—æˆåŠŸï¼Œeval_text_quality é€šè¿‡ç‡: {pass_rate['pass_rate']:.1f}%")
    
    # æ¯”è¾ƒæ‰€æœ‰ç®—å­
    pass_rates_df = eval_analyzer.compare_operators(threshold=0.7)
    print(f"âœ… ç®—å­æ¯”è¾ƒæˆåŠŸï¼ŒåŒ…å« {len(pass_rates_df)} ä¸ªè¯„ä¼°åˆ—")
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    with tempfile.TemporaryDirectory() as tmpdir:
        eval_analyzer.generate_evaluation_report(tmpdir)
        
        # æ£€æŸ¥æŠ¥å‘Šæ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
        report_files = os.listdir(tmpdir)
        expected_files = ["pass_rates.csv", "evaluation_stats.csv"]
        for file in expected_files:
            if file in report_files:
                print(f"âœ… {file} ç”ŸæˆæˆåŠŸ")
            else:
                print(f"âŒ {file} ç”Ÿæˆå¤±è´¥")
    
    # åˆ†æç®—å­å½±å“
    operator_columns = ["eval_text_quality", "eval_readability", "eval_coherence"]
    impact = eval_analyzer.analyze_operator_impact(operator_columns, "original_quality")
    print(f"âœ… ç®—å­å½±å“åˆ†ææˆåŠŸï¼Œåˆ†æäº† {len(impact)} ä¸ªç®—å­")
    
    print("ğŸ‰ EvaluationAnalyzeræµ‹è¯•é€šè¿‡ï¼")

# ä¸»æµ‹è¯•å‡½æ•°
def main():
    print("ğŸš€ å¼€å§‹æµ‹è¯•analysisåŒ…...")
    
    try:
        test_data_analyzer()
        test_data_visualizer()
        test_evaluation_analyzer()
        
        print("\nğŸŠ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼analysisåŒ…åŠŸèƒ½æ­£å¸¸ã€‚")
        return True
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()